{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d433b0-92be-4fbb-ad7d-4fb97672cfe1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using map():\n+---------+---------+\n|UpperName|DoubleAge|\n+---------+---------+\n|    ALICE|       50|\n|      BOB|       60|\n|  CHARLIE|       70|\n+---------+---------+\n\nUsing mapPartitions():\n+---------+---------+\n|UpperName|DoubleAge|\n+---------+---------+\n|    ALICE|       50|\n|      BOB|       60|\n|  CHARLIE|       70|\n+---------+---------+\n\nUsing mapPartitionsWithIndex():\n+--------------+---------+---------+\n|PartitionIndex|UpperName|DoubleAge|\n+--------------+---------+---------+\n|             2|    ALICE|       50|\n|             5|      BOB|       60|\n|             7|  CHARLIE|       70|\n+--------------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "840bb2e1-ebf4-4dea-aba3-c86881b7497d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def map_function(row):\n",
    "    name, age = row\n",
    "    return (name.upper(), age * 2)\n",
    "\n",
    "# Use map() to transform each row\n",
    "mapped_df = df.rdd.map(map_function).toDF([\"UpperName\", \"DoubleAge\"])\n",
    "print(\"Using map():\")\n",
    "mapped_df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33246e91-d71e-40ae-87c1-5eb056f83b8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to transform a partition using mapPartitions()\n",
    "def map_partitions_function(iter):\n",
    "    for row in iter:\n",
    "        yield (row[0].upper(), row[1] * 2)\n",
    "\n",
    "# Use mapPartitions() to transform each partition\n",
    "mapped_partitions_df = df.rdd.mapPartitions(map_partitions_function).toDF([\"UpperName\", \"DoubleAge\"])\n",
    "# Show the result\n",
    "print(\"Using mapPartitions():\")\n",
    "mapped_partitions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cf5f150-c5d4-436b-a556-eae51967e29d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def map_partitions_with_index_function(index, iter):\n",
    "    for row in iter:\n",
    "        yield (index, row[0].upper(), row[1] * 2)\n",
    "\n",
    "mapped_partitions_with_index_df = df.rdd.mapPartitionsWithIndex(map_partitions_with_index_function).toDF([\"PartitionIndex\", \"UpperName\", \"DoubleAge\"])\n",
    "\n",
    "mapped_partitions_with_index_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3953fc-4d8c-4705-96d8-23e398467a85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "mapped_rdd = rdd.map(lambda x: x ** 2)\n",
    "print(mapped_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6108c5-afeb-431a-a748-dd74406345bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36]\n"
     ]
    }
   ],
   "source": [
    "data1 = [[1, 2], [3, 4, 5], [6]]\n",
    "rdd2 = spark.sparkContext.parallelize(data1)\n",
    "flat_mapped_rdd = rdd2.flatMap(lambda x: [item ** 2 for item in x])\n",
    "print(flat_mapped_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb1b9db-5523-4614-98d9-6750ed6110a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('James','Smith','M',3000),\n",
    "  ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "def reformat(partitionData):\n",
    "    for row in partitionData:\n",
    "        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8c41d8-5fe9-483d-a1ab-ee99428ccfe3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n|           name|bonus|\n+---------------+-----+\n|    James,Smith|300.0|\n|      Anna,Rose|410.0|\n|Robert,Williams|620.0|\n+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df2=df.rdd.mapPartitions(reformat).toDF([\"name\",\"bonus\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85c8e07-c6ab-4765-a129-b26ce444f5b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RDD.map()\n",
    "# RDD.flatMap()\n",
    "# RDD.mapPartitions()\n",
    "# RDD.mapPartitionsWithSplit()\n",
    "# RDDBarrier.mapPartitionsWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20eb0b5a-077a-4588-83e1-ebe7dc8c1bda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "map vs mappartion vs mappartionindex",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
