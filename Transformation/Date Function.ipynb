{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24baad8c-7a6d-4764-bf72-26381b552561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+\n|current_timestamp|last_day_previous_month|\n+-----------------+-----------------------+\n|null             |null                   |\n+-----------------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_add, expr\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Example data\n",
    "data = [(\"11-11-2023 08:30:00\",)]\n",
    "columns = [\"current_timestamp\"]\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert the timestamp string to a timestamp type\n",
    "df = df.withColumn(\"current_timestamp\", col(\"current_timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Calculate the last day of the previous month\n",
    "df_result = df.withColumn(\n",
    "    \"last_day_previous_month\",\n",
    "    expr(\"date_add(last_day(current_timestamp), -1)\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2a44047-77c6-4faa-a429-a82096696c8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----------+\n|current_timestamp  |current_date|edited_date|\n+-------------------+------------+-----------+\n|11-11-2023 08:30:00|2023-12-13  |2023-11-13 |\n+-------------------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "data = [(\"11-11-2023 08:30:00\",)]\n",
    "columns = [\"current_timestamp\"]\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Add a column for the current date\n",
    "df_result = df.withColumn(\"current_date\", current_date())\n",
    "\n",
    "df_result = df_result.withColumn(\n",
    "    \"edited_date\",\n",
    "    expr(\"date_sub(current_date, 30)\")  # Subtracting approximately 30 days as an example\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad72b205-88d4-4be3-94ce-50a41071bdd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def date_of_previous month():\n",
    "    return df_result.withColumn(\"edited_date\",expr(\"date_sub(current_date, 30)\"))\n",
    "def str_to_date():\n",
    "    return df.withColumn(\"current_date\", expr(f\"to_date({colName}, {formatt})\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9acd37e4-c3c2-4839-833c-88735413b36f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n|current_timestamp  |edited_date|\n+-------------------+-----------+\n|2023-11-11 08:30:00|2023-10-12 |\n+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"2023-11-11 08:30:00\",)]\n",
    "columns = [\"current_timestamp\"]\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert the timestamp string to a timestamp type\n",
    "df = df.withColumn(\"current_timestamp\", col(\"current_timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Subtract 1 month from the current timestamp using expr\n",
    "df_result = df.withColumn(\n",
    "    \"edited_date\",\n",
    "    expr(\"date_sub(current_timestamp, 30)\")  # Subtracting approximately 30 days as an example\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9edcd6-f3ff-4442-b2f8-cdb8f330adf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n|current_date|edited_date|\n+------------+-----------+\n|2025-01-24  |2024-12-31 |\n+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, expr, when, last_day\n",
    "data = [(\"24-01-2025\",)]\n",
    "columns = [\"current_date\"]\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert the date string to a date type\n",
    "df = df.withColumn(\"current_date\", expr(\"to_date(current_date, 'dd-MM-yyyy')\"))\n",
    "\n",
    "# Edit the current date based on the specified conditions\n",
    "df_result = df.withColumn(\n",
    "    \"edited_date\",\n",
    "    when(expr(\"month(current_date) = 1\"),\n",
    "         last_day(expr(\"add_months(current_date, -1)\")))\n",
    "    .otherwise(df[\"current_date\"])\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db496c0-9750-4d79-9c28-99419668757a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-1756207835532921>\", line 7, in <module>\n    df = df.withColumn(\"current_date\", expr(\"to_date(given_date, 'dd-MM-yyyy')\"))\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/dataframe.py\", line 4758, in withColumn\n    return DataFrame(self._jdf.withColumn(colName, col._jc), self.sparkSession)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 234, in deco\n    raise converted from None\npyspark.errors.exceptions.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `given_date` cannot be resolved. Did you mean one of the following? [`current_date`].; line 1 pos 8;\n'Project ['to_date('given_date, dd-MM-yyyy) AS current_date#135]\n+- LogicalRDD [current_date#133], false\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `given_date` cannot be resolved. Did you mean one of the following? [`current_date`].; line 1 pos 8;\n'Project ['to_date('given_date, dd-MM-yyyy) AS current_date#135]\n+- LogicalRDD [current_date#133], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example data\n",
    "data = [(\"12-01-2024\",)]\n",
    "columns = [\"current_date\"]\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"current_date\", expr(\"to_date(given_date, 'dd-MM-yyyy')\"))\n",
    "# Edit the current date based on the specified conditions\n",
    "df_result = df.withColumn(\n",
    "    \"edited_date\",\n",
    "    expr(\"when(month(current_date) = 1, last_day(current_date - day(current_date))).otherwise(current_date)\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961de37d-7975-4ffd-b71f-a2dc84766fbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n|given_date|last_day_previous_month|\n+----------+-----------------------+\n|2024-01-12|2023-12-31             |\n+----------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(\"12-01-2024\",)]\n",
    "columns = [\"given_date\"]\n",
    "# Create a PySpark DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# Convert the date string to a date type\n",
    "df = df.withColumn(\"given_date\", expr(\"to_date(given_date, 'dd-MM-yyyy')\"))\n",
    "# Calculate the last day of the previous month for the given date\n",
    "df_result = df.withColumn(\n",
    "    \"last_day_previous_month\",\n",
    "    expr(\"last_day(add_months(given_date, -1))\")\n",
    ")\n",
    "# Show the result\n",
    "df_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbaf732e-c0db-45af-a818-be864b43ff4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ss(a,b):\n",
    "    return a+b"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Date Function",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
