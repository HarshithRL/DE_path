{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b90092c6-7837-4f0c-b698-39978ebb41b8",
     "showTitle": true,
     "title": "Problem Statement"
    }
   },
   "source": [
    "There is a monthly pipeline which will run monthly once. \n",
    "Each month we have to store the data which we are getting to the table.  \n",
    "The dataframe consisting of 4 columns namely employee_id, employee_name, employee_age, and month among which employee_id is unique. \n",
    "The table should contain 5 columns namely employee_id, employee_name, employee_age, month, and status.   \n",
    "Status column can contain 3 kinds of values 1. Additional, 2. Existing, 3. Missing.  \n",
    "  *  If we get new employee_id then, the value of the status column should be Additional  \n",
    "  *  If we get the same employee_id next month then, the value of the status column should be Existing  \n",
    "  *  If the employee_id is missing in the next month then the value of the status column should be Missing. \n",
    "\n",
    "--------------------final output dataframe------------------------  \n",
    "\n",
    "|-------------|-------------|--------------|----------|----------|\n",
    "|employee_id |employee_name | employee_age | month_no |status    |\n",
    "|------------|--------------|--------------|----------|----------|\n",
    "|      dbg01 |Monkey D luffy| 21           | 4        |Existing  |\n",
    "|      dbg02 |Ronovara zoro | 23           | 3        |Missing   |\n",
    "|      dbg03 |prince sanji  | 23           | 4        |Existing  |\n",
    "|      dbg04 |nami          | 24           | 2        |Missing   |\n",
    "|      dbg05 |ussopu        | 19           | 4        |Additional|\n",
    "|------------|--------------|--------------|----------|----------|\n",
    "\n",
    "###Hint: If the id matches then, the entire row should be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e8c43e-9299-419e-a14c-f99b0de535f9",
     "showTitle": true,
     "title": "Import Statements"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import lit,col,expr\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4fef3b-9a6f-4d4e-8b32-2035bce16973",
     "showTitle": true,
     "title": "Month 1 dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+--------+\n|employee_id|employee_name|employee_age|month_no|\n+-----------+-------------+------------+--------+\n|      dgb01|        luffy|          20|       1|\n|      dgb02|         zoro|          22|       1|\n|      dgb03|        sanji|          22|       1|\n+-----------+-------------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "data_month_1 = [\n",
    "  ('dgb01', 'luffy', 20, 1), \n",
    "  ('dgb02', 'zoro', 22, 1),\n",
    "  ('dgb03', 'sanji', 22, 1)\n",
    "        ]\n",
    "schema = StructType([\n",
    "  StructField('employee_id', StringType(), True),\n",
    "  StructField('employee_name', StringType(), True),\n",
    "  StructField('employee_age', IntegerType(), True),\n",
    "  StructField('month_no', IntegerType(), True)\n",
    "])\n",
    "df_month_1 = spark.createDataFrame(data_month_1, schema)\n",
    "df_month_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576a8229-ecde-45c3-97b7-8d7f4781258d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_month_1.write.mode(\"overwrite\").format(\"delta\").option(\"path\",dbname).saveAsTable(f'{dbname}.{table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df41c9dd-acb2-4069-8368-f3e0f786475b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WRITE YOUR SAVE LOGIC HERE ..\n",
    "def mergeAndSaveDf(df,path,database_name,target_table_name, mergeCol):\n",
    "    d1 = dict(map(lambda x: (f'target.{x}', f'source.{x}'), df.columns))\n",
    "    d2=dict(map(lambda x: (f'target.{x}', f'source.{x}'), df.columns))\n",
    "    d1['target.status']=lit('Existing')\n",
    "    d2['target.status']=lit('Additional')\n",
    "    if DeltaTable.isDeltaTable(spark,f\"{path}\"):\n",
    "        deltaTable = DeltaTable.forPath(spark, f\"{path}\")\n",
    "        spark.sql(f\"UPDATE {database_name}.{target_table_name} SET status='missing'\")\n",
    "        deltaTable.alias(\"target\").merge(\n",
    "          source=df.alias(\"source\"),\n",
    "          condition=f'target.{mergeCol}=source.{mergeCol}').whenMatchedUpdate(set=d1).whenNotMatchedInsert(values=d2).execute()\n",
    "    else:\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        df_final = df.withColumn(\"status\",lit(\"Additional\"))\n",
    "        df_final.write.mode(\"overwrite\").format(\"delta\").option(\"path\",path).saveAsTable(f\"{database_name}.{target_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40c6747-224e-42c7-9ddf-33aebbac357f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergeAndSaveDf(df_month_1,'dbfs:/FileStore/delta_table2','dbo2','my_view1','employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3f67498-de63-4c06-b12b-26464212ac55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>employee_name</th><th>employee_age</th><th>month_no</th><th>status</th></tr></thead><tbody><tr><td>dgb01</td><td>luffy</td><td>20</td><td>1</td><td>Additional</td></tr><tr><td>dgb03</td><td>sanji</td><td>22</td><td>1</td><td>Additional</td></tr><tr><td>dgb02</td><td>zoro</td><td>22</td><td>1</td><td>Additional</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dgb01",
         "luffy",
         20,
         1,
         "Additional"
        ],
        [
         "dgb03",
         "sanji",
         22,
         1,
         "Additional"
        ],
        [
         "dgb02",
         "zoro",
         22,
         1,
         "Additional"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_no",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dbo2.my_view1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bb7ebd-abeb-4fb4-af6c-8e4d054d2f25",
     "showTitle": true,
     "title": "save first time "
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4076083443574051>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-4076083443574051>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mZHJvcCBkYXRhYmFzZSBkYm8=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [SCHEMA_NOT_FOUND] The schema `dbo` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\n",
       "To tolerate the error on drop use DROP SCHEMA IF EXISTS."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4076083443574051>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-4076083443574051>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mZHJvcCBkYXRhYmFzZSBkYm8=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [SCHEMA_NOT_FOUND] The schema `dbo` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [SCHEMA_NOT_FOUND] The schema `dbo` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "drop database dbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c544fbb-a045-4da2-a0e2-f8c8fc95e3cb",
     "showTitle": true,
     "title": "Month 2 dataframe"
    }
   },
   "outputs": [],
   "source": [
    "data_month_2 = [\n",
    "  ('dgb01', 'Monkey D luffy', 21, 2), \n",
    "  ('dgb04', 'nami', 24, 2)\n",
    "        ]\n",
    "        \n",
    "df_month_2 = spark.createDataFrame(data_month_2, schema)\n",
    "\n",
    "# call your merge method mergeAndSaveDf(df_month_2 , tgtTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2ed3656-38be-41bf-9087-cd843d12363f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergeAndSaveDf(df_month_2,'dbfs:/FileStore/delta_table2','dbo2','my_view1','employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1279420-2f94-4d21-9bcb-45e02c7cf8db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>employee_name</th><th>employee_age</th><th>month_no</th><th>status</th></tr></thead><tbody><tr><td>dgb01</td><td>Monkey D luffy</td><td>21</td><td>2</td><td>Existing</td></tr><tr><td>dgb04</td><td>nami</td><td>24</td><td>2</td><td>Additional</td></tr><tr><td>dgb03</td><td>sanji</td><td>22</td><td>1</td><td>missing</td></tr><tr><td>dgb02</td><td>zoro</td><td>22</td><td>1</td><td>missing</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dgb01",
         "Monkey D luffy",
         21,
         2,
         "Existing"
        ],
        [
         "dgb04",
         "nami",
         24,
         2,
         "Additional"
        ],
        [
         "dgb03",
         "sanji",
         22,
         1,
         "missing"
        ],
        [
         "dgb02",
         "zoro",
         22,
         1,
         "missing"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_no",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--dbutils.fs.rm('dbfs:/FileStore/delta_table2',True)\n",
    "select * from dbo2.my_view1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d981aad-a4b6-4674-a0e1-4a4daa7eba77",
     "showTitle": true,
     "title": "Month 3 dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+--------+\n|employee_id|employee_name|employee_age|month_no|\n+-----------+-------------+------------+--------+\n|      dgb02|Ronovara zoro|          23|       3|\n+-----------+-------------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "data_month_3 = [\n",
    "  ('dgb02', 'Ronovara zoro', 23, 3),\n",
    "        ]\n",
    "df_month_3 = spark.createDataFrame(data_month_3, schema)\n",
    "df_month_3.show()\n",
    "\n",
    "#call your merge method mergeAndSaveDf(df_month_3 , tgtTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f183bee7-ede1-4e97-9449-6dcf69fdea49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergeAndSaveDf(df_month_3,'dbfs:/FileStore/delta_table2','dbo2','my_view1','employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f938b9-1dcb-4608-8bd6-9ca1d3ad7559",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>employee_name</th><th>employee_age</th><th>month_no</th><th>status</th></tr></thead><tbody><tr><td>dgb01</td><td>Monkey D luffy</td><td>21</td><td>2</td><td>missing</td></tr><tr><td>dgb02</td><td>Ronovara zoro</td><td>23</td><td>3</td><td>Existing</td></tr><tr><td>dgb03</td><td>sanji</td><td>22</td><td>1</td><td>missing</td></tr><tr><td>dgb04</td><td>nami</td><td>24</td><td>2</td><td>missing</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dgb01",
         "Monkey D luffy",
         21,
         2,
         "missing"
        ],
        [
         "dgb02",
         "Ronovara zoro",
         23,
         3,
         "Existing"
        ],
        [
         "dgb03",
         "sanji",
         22,
         1,
         "missing"
        ],
        [
         "dgb04",
         "nami",
         24,
         2,
         "missing"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_no",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dbo2.my_view1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f36cf0-720a-46e2-b36b-87fd9d49ac05",
     "showTitle": true,
     "title": "Month 4 dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------+--------+\n|employee_id| employee_name|employee_age|month_no|\n+-----------+--------------+------------+--------+\n|      dgb01|Monkey D luffy|          21|       4|\n|      dgb03|  prince sanji|          23|       4|\n|      dgb05|        ussopu|          19|       4|\n+-----------+--------------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "data_month_4 = [\n",
    "  ('dgb01', 'Monkey D luffy', 21, 4), \n",
    "  ('dgb03', 'prince sanji', 23, 4),\n",
    "  ('dgb05', 'ussopu', 19, 4),\n",
    "        ]\n",
    "df_month_4 = spark.createDataFrame(data_month_4, schema)\n",
    "df_month_4.show()\n",
    "\n",
    "#call your merge method mergeAndSaveDf(df_month_4 , tgtTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ae5e12-277e-4ffc-9036-b358a70df204",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergeAndSaveDf(df_month_4,'dbfs:/FileStore/delta_table2','dbo2','my_view1','employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60ff5e2-2726-459e-9a03-babaeb144b8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>employee_name</th><th>employee_age</th><th>month_no</th><th>status</th></tr></thead><tbody><tr><td>dgb01</td><td>Monkey D luffy</td><td>21</td><td>4</td><td>Existing</td></tr><tr><td>dgb02</td><td>Ronovara zoro</td><td>23</td><td>3</td><td>missing</td></tr><tr><td>dgb03</td><td>prince sanji</td><td>23</td><td>4</td><td>Existing</td></tr><tr><td>dgb05</td><td>ussopu</td><td>19</td><td>4</td><td>Additional</td></tr><tr><td>dgb04</td><td>nami</td><td>24</td><td>2</td><td>missing</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dgb01",
         "Monkey D luffy",
         21,
         4,
         "Existing"
        ],
        [
         "dgb02",
         "Ronovara zoro",
         23,
         3,
         "missing"
        ],
        [
         "dgb03",
         "prince sanji",
         23,
         4,
         "Existing"
        ],
        [
         "dgb05",
         "ussopu",
         19,
         4,
         "Additional"
        ],
        [
         "dgb04",
         "nami",
         24,
         2,
         "missing"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_no",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dbo2.my_view1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf26b250-dd11-490d-89b1-b029c9af80be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write Assert Logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37cc6c7e-2212-48c8-9560-e0e59656eb1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_month_4 = [\n",
    "  ('dgb01', 'Monkey D luffy', 21, 4), \n",
    "  ('dgb03', 'prince sanji', 23, 4),\n",
    "  ('dgb05', 'ussopu', 19, 4),\n",
    "  ('dgb02', 'Ronovara zoro', 23, 3)\n",
    "        ]\n",
    "df_1 = spark.createDataFrame(data_month_4, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0ab002-fddd-4e9e-b70b-2f31f2482d54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergeAndSaveDf(df_month_4,'dbfs:/FileStore/delta_table2','dbo2','my_view1','employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61498cfc-1cc1-4b52-b8da-82454de81277",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfda421-b44d-444b-80ab-e0280b34c384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+--------+----+\n|employee_id|employee_name|employee_age|month_no|Flag|\n+-----------+-------------+------------+--------+----+\n|      dgb01|        luffy|          20|       1|   A|\n|      dgb02|         zoro|          22|       1|   A|\n|      dgb03|        sanji|          22|       1|   A|\n+-----------+-------------+------------+--------+----+\n\n"
     ]
    }
   ],
   "source": [
    "data_month_1 = [\n",
    "  ('dgb01', 'luffy', 20, 1,'A'), \n",
    "  ('dgb02', 'zoro', 22, 1,'A'),\n",
    "  ('dgb03', 'sanji', 22, 1,'A')\n",
    "        ]\n",
    "schema = StructType([\n",
    "  StructField('employee_id', StringType(), True),\n",
    "  StructField('employee_name', StringType(), True),\n",
    "  StructField('employee_age', IntegerType(), True),\n",
    "  StructField('month_no', IntegerType(), True),\n",
    "  StructField('Flag', StringType(), True)\n",
    "])\n",
    "df_month_1 = spark.createDataFrame(data_month_1, schema)\n",
    "df_month_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc2e7ce-74a5-4fad-b829-85cf9333115f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_month_1.write.mode(\"overwrite\").format(\"delta\").option(\"path\",'dbfs:/FileStore/delta_table3').saveAsTable('scd2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57dfef38-d4c8-412d-bd34-053d71f0eb26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>employee_name</th><th>employee_age</th><th>month_no</th><th>Flag</th></tr></thead><tbody><tr><td>dgb01</td><td>luffy</td><td>20</td><td>1</td><td>A</td></tr><tr><td>dgb03</td><td>sanji</td><td>22</td><td>1</td><td>A</td></tr><tr><td>dgb02</td><td>zoro</td><td>22</td><td>1</td><td>A</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dgb01",
         "luffy",
         20,
         1,
         "A"
        ],
        [
         "dgb03",
         "sanji",
         22,
         1,
         "A"
        ],
        [
         "dgb02",
         "zoro",
         22,
         1,
         "A"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_no",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Flag",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from scd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99cce6c1-5bbf-48ea-b17a-a0e88d88e113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customersTable\n",
    "  .as(\"customers\")\n",
    "  .merge(\n",
    "    stagedUpdates.as(\"staged_updates\"),\n",
    "    \"customers.customerId = mergeKey\")\n",
    "  .whenMatched(\"customers.current = true AND customers.address <> staged_updates.address\")\n",
    "  .updateExpr(Map(                                      #Set current to false and endDate to source's effective date.\n",
    "    \"current\" -> \"false\",\n",
    "    \"endDate\" -> \"staged_updates.effectiveDate\"))\n",
    "  .whenNotMatched()\n",
    "  .insertExpr(Map(\n",
    "    \"customerid\" -> \"staged_updates.customerId\",\n",
    "    \"address\" -> \"staged_updates.address\",\n",
    "    \"current\" -> \"true\",\n",
    "    \"effectiveDate\" -> \"staged_updates.effectiveDate\",  #Set current to true along with the new address and its effective date.\n",
    "    \"endDate\" -> \"null\"))\n",
    "  .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5fb32c-137b-4c0c-8773-f2952954a909",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTable_main = DeltaTable.forPath(spark, 'dbfs:/FileStore/delta_table3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5827cc11-700e-41d0-b178-b7a448b6f344",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+--------+\n|employee_id|employee_name|employee_age|month_no|\n+-----------+-------------+------------+--------+\n|      dgb01|        luffy|          20|       1|\n|      dgb02|    lost zoro|          22|       1|\n|      dgb04|          Ace|          22|       1|\n+-----------+-------------+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "data_month_2 = [\n",
    "  ('dgb01', 'luffy', 20, 1), \n",
    "  ('dgb02', 'lost zoro', 22, 1),\n",
    "  ('dgb04', 'Ace', 22, 1)\n",
    "        ]\n",
    "schema = StructType([\n",
    "  StructField('employee_id', StringType(), True),\n",
    "  StructField('employee_name', StringType(), True),\n",
    "  StructField('employee_age', IntegerType(), True),\n",
    "  StructField('month_no', IntegerType(), True)\n",
    "])\n",
    "df_month_2 = spark.createDataFrame(data_month_2, schema)\n",
    "df_month_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36eea052-966d-431c-b574-ef57456f1f76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[65]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm('dbfs:/FileStore/delta_table2',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b7c387-cb46-442a-97c7-b9ed32b4aaf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-300362380996602>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdeltaTable\u001B[49m\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n",
       "\u001B[1;32m      2\u001B[0m          source\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      3\u001B[0m          condition\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmergeCol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m=source.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmergeCol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mwhenMatchedUpdate(\u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39md1)\u001B[38;5;241m.\u001B[39mwhenNotMatchedInsert(values\u001B[38;5;241m=\u001B[39md2)\u001B[38;5;241m.\u001B[39mexecute()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'deltaTable' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-300362380996602>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdeltaTable\u001B[49m\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmerge(\n\u001B[1;32m      2\u001B[0m          source\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      3\u001B[0m          condition\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmergeCol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m=source.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmergeCol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mwhenMatchedUpdate(\u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39md1)\u001B[38;5;241m.\u001B[39mwhenNotMatchedInsert(values\u001B[38;5;241m=\u001B[39md2)\u001B[38;5;241m.\u001B[39mexecute()\n\n\u001B[0;31mNameError\u001B[0m: name 'deltaTable' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'deltaTable' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " deltaTable.alias(\"target\").merge(\n",
    "          source=df.alias(\"source\"),\n",
    "          condition=f'target.{mergeCol}=source.{mergeCol}').whenMatchedUpdate(set=d1).whenNotMatchedInsert(values=d2).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8155dfa9-e919-4c73-ba58-3bb1ae741ee5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTable_main.alias(\"target\").merge(\n",
    "    stagedUpdates.alias(\"source\"),\n",
    "    \"target.employee_id = mergeKey\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"target.Flag = 'A' AND target.employee_name != source.employee_name\",\n",
    "    set={\n",
    "        \"Flag\": lit(\"aa\")  # Set current to false and endDate to source's effective date.\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"employee_id\": \"source.employee_id\",\n",
    "        \"employee_name\": \"source.employee_name\",\n",
    "        \"employee_age\": \"source.employee_age\",\n",
    "        \"month_no\": \"source.month_no\",\n",
    "        \"Flag\": lit(\"A\")  # Set current to true along with the new address and its effective date.\n",
    "    }\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c797cb3d-49bd-4482-9f8b-d20137723ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>employee_name</th><th>employee_age</th><th>month_no</th><th>Flag</th></tr></thead><tbody><tr><td>dgb02</td><td>lost zoro</td><td>22</td><td>1</td><td>A</td></tr><tr><td>dgb01</td><td>luffy</td><td>20</td><td>1</td><td>A</td></tr><tr><td>dgb02</td><td>zoro</td><td>22</td><td>1</td><td>aa</td></tr><tr><td>dgb03</td><td>sanji</td><td>22</td><td>1</td><td>A</td></tr><tr><td>dgb04</td><td>Ace</td><td>22</td><td>1</td><td>A</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dgb02",
         "lost zoro",
         22,
         1,
         "A"
        ],
        [
         "dgb01",
         "luffy",
         20,
         1,
         "A"
        ],
        [
         "dgb02",
         "zoro",
         22,
         1,
         "aa"
        ],
        [
         "dgb03",
         "sanji",
         22,
         1,
         "A"
        ],
        [
         "dgb04",
         "Ace",
         22,
         1,
         "A"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_no",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Flag",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc820ee8-a957-4a07-a039-95bcecd86ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "newAddressesToInsert = df_month_2 \\\n",
    "  .alias(\"updates\") \\\n",
    "  .join(deltaTable_main.toDF().alias(\"main\"), \"employee_id\") \\\n",
    "  .where(\"main.Flag = 'A' AND updates.employee_name <> main.employee_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deee52d0-2c13-4fbb-b364-735d5b4ea812",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[employee_id: string, employee_name: string, employee_age: int, month_no: int, employee_name: string, employee_age: int, month_no: int, Flag: string]\n"
     ]
    }
   ],
   "source": [
    "print(newAddressesToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713f9893-273d-481a-ac14-bd29a4bcde02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-300362380996614>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mnewAddressesToInsert\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/monkey_patches.py:43\u001B[0m, in \u001B[0;36mapply_dataframe_display_patch.<locals>.df_display\u001B[0;34m(df, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdf_display\u001B[39m(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m     40\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    df.display() is an alias for display(df). Run help(display) for more information.\u001B[39;00m\n",
       "\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m---> 43\u001B[0m     \u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/display.py:83\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     80\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
       "\u001B[1;32m     81\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTriggers can only be set for streaming queries.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m---> 83\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_custom_display_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[1;32m     86\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28minput\u001B[39m))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/display.py:36\u001B[0m, in \u001B[0;36mDisplay.add_custom_display_data\u001B[0;34m(self, data_type, data)\u001B[0m\n",
       "\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_custom_display_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_type, data):\n",
       "\u001B[1;32m     35\u001B[0m     custom_display_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(uuid\u001B[38;5;241m.\u001B[39muuid4())\n",
       "\u001B[0;32m---> 36\u001B[0m     return_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddCustomDisplayData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcustom_display_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     37\u001B[0m     ip_display({\n",
       "\u001B[1;32m     38\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.v1+display\u001B[39m\u001B[38;5;124m\"\u001B[39m: custom_display_key,\n",
       "\u001B[1;32m     39\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<Databricks Output (not supported in output widgets)>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     40\u001B[0m     },\n",
       "\u001B[1;32m     41\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: The schema of your Delta table has changed in an incompatible way since your DataFrame\n",
       "or DeltaTable object was created. Please redefine your DataFrame or DeltaTable object.\n",
       "Changes:\n",
       "Latest schema is missing field(s): employee_id, month_no, employee_name, Flag, employee_age"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-300362380996614>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mnewAddressesToInsert\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/monkey_patches.py:43\u001B[0m, in \u001B[0;36mapply_dataframe_display_patch.<locals>.df_display\u001B[0;34m(df, *args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdf_display\u001B[39m(df, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03m    df.display() is an alias for display(df). Run help(display) for more information.\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m     \u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/display.py:83\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrigger\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     81\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTriggers can only be set for streaming queries.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 83\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_custom_display_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28minput\u001B[39m))\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/display.py:36\u001B[0m, in \u001B[0;36mDisplay.add_custom_display_data\u001B[0;34m(self, data_type, data)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_custom_display_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_type, data):\n\u001B[1;32m     35\u001B[0m     custom_display_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(uuid\u001B[38;5;241m.\u001B[39muuid4())\n\u001B[0;32m---> 36\u001B[0m     return_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mentry_point\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maddCustomDisplayData\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcustom_display_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m     ip_display({\n\u001B[1;32m     38\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.v1+display\u001B[39m\u001B[38;5;124m\"\u001B[39m: custom_display_key,\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<Databricks Output (not supported in output widgets)>\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     40\u001B[0m     },\n\u001B[1;32m     41\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m return_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: The schema of your Delta table has changed in an incompatible way since your DataFrame\nor DeltaTable object was created. Please redefine your DataFrame or DeltaTable object.\nChanges:\nLatest schema is missing field(s): employee_id, month_no, employee_name, Flag, employee_age",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: The schema of your Delta table has changed in an incompatible way since your DataFrame\nor DeltaTable object was created. Please redefine your DataFrame or DeltaTable object.\nChanges:\nLatest schema is missing field(s): employee_id, month_no, employee_name, Flag, employee_age",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "newAddressesToInsert.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ca34de-4bee-4167-bd8e-669a70b2e8b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stagedUpdates = (\n",
    "  newAddressesToInsert\n",
    "  .selectExpr(\"NULL as mergeKey\", \"updates.*\")   # Rows for 1\n",
    "  .union(df_month_2.selectExpr(\"employee_id as mergeKey\", \"*\"))  # Rows for 2.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b3211a-8458-4719-8c14-279c8da4d779",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mergeKey</th><th>employee_id</th><th>employee_name</th><th>employee_age</th><th>month_no</th></tr></thead><tbody><tr><td>dgb01</td><td>dgb01</td><td>luffy</td><td>20</td><td>1</td></tr><tr><td>dgb02</td><td>dgb02</td><td>lost zoro</td><td>22</td><td>1</td></tr><tr><td>dgb04</td><td>dgb04</td><td>Ace</td><td>22</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dgb01",
         "dgb01",
         "luffy",
         20,
         1
        ],
        [
         "dgb02",
         "dgb02",
         "lost zoro",
         22,
         1
        ],
        [
         "dgb04",
         "dgb04",
         "Ace",
         22,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "mergeKey",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "employee_age",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month_no",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "stagedUpdates.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01f6ca5-1a32-43db-b355-e1776825ebb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 300362380996600,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "SCD 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
